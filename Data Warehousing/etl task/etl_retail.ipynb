{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d98b6bb",
   "metadata": {},
   "source": [
    "# Task 2: ETL Process Implementation\n",
    "DSA 2040 - Data Warehousing and Data Mining Practical Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923b1b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Process Implementation for Retail Data Warehouse\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ETL Process Implementation for Retail Data Warehouse\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781d5e1",
   "metadata": {},
   "source": [
    "## Step 1: Data Generation (Extract Phase)\n",
    "Since we're generating synthetic data, we'll create realistic retail transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc30771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 13:43:23,170 - INFO - Starting data generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Extracting Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 13:43:23,534 - INFO - Generated 1000 rows of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 1000 rows of synthetic retail data\n",
      "‚úì Data saved to 'synthetic_retail_data.csv'\n",
      "\n",
      "Extracted Data Summary:\n",
      "Shape: (1000, 9)\n",
      "Columns: ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'customer_id', 'Country', 'Category']\n",
      "\n",
      "First 5 rows:\n",
      "  InvoiceNo StockCode          Description  Quantity         InvoiceDate  \\\n",
      "0    536366   CL31454      Jacket Clothing        39 2023-09-12 14:31:01   \n",
      "1    536366   EL85228  Speaker Electronics         4 2025-03-22 08:31:35   \n",
      "2    536366   BO71516         Manual Books         4 2024-03-21 06:30:37   \n",
      "3    536366   SP51195         Towel Sports        33 2024-08-10 02:12:58   \n",
      "4    536367   SP50955       Weights Sports        48 2023-10-05 02:09:26   \n",
      "\n",
      "   UnitPrice  customer_id  Country     Category  \n",
      "0      63.90     12396.0   France     Clothing  \n",
      "1      35.85     12417.0    Spain  Electronics  \n",
      "2      67.56     12388.0  Germany        Books  \n",
      "3      95.23     12351.0    Spain       Sports  \n",
      "4      95.88     12397.0      USA       Sports  \n",
      "\n",
      "Data types:\n",
      "InvoiceNo              object\n",
      "StockCode              object\n",
      "Description            object\n",
      "Quantity                int64\n",
      "InvoiceDate    datetime64[ns]\n",
      "UnitPrice             float64\n",
      "customer_id            float64\n",
      "Country                object\n",
      "Category               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_retail_data(num_rows=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic retail data similar to Online Retail dataset structure\n",
    "    Columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, customer_id, Country\n",
    "    \"\"\"\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Define product categories and items\n",
    "    categories = {\n",
    "        'Electronics': ['Smartphone', 'Laptop', 'Tablet', 'Headphones', 'Speaker', 'Charger'],\n",
    "        'Clothing': ['T-Shirt', 'Jeans', 'Dress', 'Jacket', 'Shoes', 'Hat'],\n",
    "        'Home': ['Chair', 'Table', 'Lamp', 'Cushion', 'Mug', 'Plate'],\n",
    "        'Books': ['Novel', 'Cookbook', 'Biography', 'Manual', 'Dictionary', 'Magazine'],\n",
    "        'Sports': ['Ball', 'Racket', 'Weights', 'Mat', 'Bottle', 'Towel']\n",
    "    }\n",
    "    \n",
    "    countries = ['UK', 'France', 'Germany', 'Netherlands', 'Spain', 'Italy', 'USA', 'Canada', 'Australia']\n",
    "    \n",
    "    # Generate data\n",
    "    data = []\n",
    "    invoice_counter = 536365  # Starting invoice number\n",
    "    customer_ids = list(range(12346, 12446))  # 100 unique customers\n",
    "    \n",
    "    # Create date range for last 2 years\n",
    "    end_date = datetime(2025, 8, 12)\n",
    "    start_date = end_date - timedelta(days=730)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        # Select category and product\n",
    "        category = random.choice(list(categories.keys()))\n",
    "        product = random.choice(categories[category])\n",
    "        \n",
    "        # Generate invoice (group some items in same invoice)\n",
    "        if i % random.randint(1, 5) == 0:\n",
    "            invoice_counter += 1\n",
    "        \n",
    "        # Generate data row\n",
    "        row = {\n",
    "            'InvoiceNo': f'{invoice_counter}',\n",
    "            'StockCode': f'{category[:2].upper()}{random.randint(10000, 99999)}',\n",
    "            'Description': f'{product} {category}',\n",
    "            'Quantity': random.randint(1, 50),\n",
    "            'InvoiceDate': fake.date_time_between(start_date=start_date, end_date=end_date),\n",
    "            'UnitPrice': round(random.uniform(1.0, 100.0), 2),\n",
    "            'customer_id': random.choice(customer_ids),\n",
    "            'Country': random.choice(countries),\n",
    "            'Category': category  # Adding category for easier analysis\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some missing values and outliers to make it realistic\n",
    "    # Missing customer_id for some rows\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'customer_id'] = np.nan\n",
    "    \n",
    "    # Add some negative quantities (returns)\n",
    "    return_indices = np.random.choice(df.index, size=int(0.03 * len(df)), replace=False)\n",
    "    df.loc[return_indices, 'Quantity'] = -df.loc[return_indices, 'Quantity']\n",
    "    \n",
    "    # Add some zero/negative prices\n",
    "    bad_price_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
    "    df.loc[bad_price_indices, 'UnitPrice'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"Step 1: Extracting Data...\")\n",
    "logger.info(\"Starting data generation\")\n",
    "\n",
    "# Generate the dataset\n",
    "retail_data = generate_synthetic_retail_data(1000)\n",
    "\n",
    "# Save to CSV for reference\n",
    "retail_data.to_csv('synthetic_retail_data.csv', index=False)\n",
    "\n",
    "print(f\"‚úì Generated {len(retail_data)} rows of synthetic retail data\")\n",
    "print(f\"‚úì Data saved to 'synthetic_retail_data.csv'\")\n",
    "logger.info(f\"Generated {len(retail_data)} rows of data\")\n",
    "\n",
    "# Display basic info about extracted data\n",
    "print(\"\\nExtracted Data Summary:\")\n",
    "print(f\"Shape: {retail_data.shape}\")\n",
    "print(f\"Columns: {list(retail_data.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(retail_data.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(retail_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ea8d5",
   "metadata": {},
   "source": [
    "## Step 2: Transform Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7e547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 13:44:56,350 - INFO - Starting data transformation\n",
      "2025-08-14 13:44:56,379 - INFO - Transformation complete. Rows: 1000 -> 467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Transforming Data...\n",
      "Initial rows: 1000\n",
      "- Handling missing values...\n",
      "  Missing values before: 50, after: 0\n",
      "- Converting data types...\n",
      "- Removing outliers...\n",
      "  Removed 47 rows with outliers\n",
      "- Calculating TotalSales...\n",
      "- Filtering for last year's data...\n",
      "  Filtered out 436 rows older than August 12, 2024\n",
      "- Creating time dimensions...\n",
      "Final transformed rows: 467\n",
      "- Creating Customer Dimension...\n",
      "  Created customer dimension with 99 customers\n",
      "- Creating Time Dimension...\n",
      "  Created time dimension with 261 time periods\n",
      "- Creating Product Dimension...\n",
      "  Created product dimension with 467 products\n",
      "\n",
      "Transformation Summary:\n",
      "‚úì Fact table rows: 467\n",
      "‚úì Customer dimension: 99 records\n",
      "‚úì Time dimension: 261 records\n",
      "‚úì Product dimension: 467 records\n"
     ]
    }
   ],
   "source": [
    "def transform_retail_data(df):\n",
    "    \"\"\"\n",
    "    Transform the retail data for data warehouse loading\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 2: Transforming Data...\")\n",
    "    logger.info(\"Starting data transformation\")\n",
    "    \n",
    "    # Create a copy for transformation\n",
    "    transformed_df = df.copy()\n",
    "    initial_rows = len(transformed_df)\n",
    "    \n",
    "    print(f\"Initial rows: {initial_rows}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"- Handling missing values...\")\n",
    "    missing_before = transformed_df.isnull().sum().sum()\n",
    "    \n",
    "    # Drop rows with missing customer_id (for this warehouse, we need customer info)\n",
    "    transformed_df = transformed_df.dropna(subset=['customer_id'])\n",
    "    transformed_df['customer_id'] = transformed_df['customer_id'].astype(int)\n",
    "    \n",
    "    missing_after = transformed_df.isnull().sum().sum()\n",
    "    print(f\"  Missing values before: {missing_before}, after: {missing_after}\")\n",
    "    \n",
    "    # Convert data types\n",
    "    print(\"- Converting data types...\")\n",
    "    transformed_df['InvoiceDate'] = pd.to_datetime(transformed_df['InvoiceDate'])\n",
    "    transformed_df['UnitPrice'] = pd.to_numeric(transformed_df['UnitPrice'], errors='coerce')\n",
    "    transformed_df['Quantity'] = pd.to_numeric(transformed_df['Quantity'], errors='coerce')\n",
    "    \n",
    "    # Handle outliers - Remove negative quantities and zero/negative prices\n",
    "    print(\"- Removing outliers...\")\n",
    "    rows_before_outliers = len(transformed_df)\n",
    "    \n",
    "    # Remove returns (negative quantities) and invalid prices\n",
    "    transformed_df = transformed_df[\n",
    "        (transformed_df['Quantity'] > 0) & \n",
    "        (transformed_df['UnitPrice'] > 0)\n",
    "    ]\n",
    "    \n",
    "    rows_after_outliers = len(transformed_df)\n",
    "    outliers_removed = rows_before_outliers - rows_after_outliers\n",
    "    print(f\"  Removed {outliers_removed} rows with outliers\")\n",
    "    \n",
    "    # Calculate TotalSales\n",
    "    print(\"- Calculating TotalSales...\")\n",
    "    transformed_df['TotalSales'] = transformed_df['Quantity'] * transformed_df['UnitPrice']\n",
    "    \n",
    "    # Filter for last year's data (from August 12, 2024)\n",
    "    print(\"- Filtering for last year's data...\")\n",
    "    cutoff_date = datetime(2024, 8, 12)\n",
    "    rows_before_filter = len(transformed_df)\n",
    "    \n",
    "    transformed_df = transformed_df[transformed_df['InvoiceDate'] >= cutoff_date]\n",
    "    \n",
    "    rows_after_filter = len(transformed_df)\n",
    "    filtered_rows = rows_before_filter - rows_after_filter\n",
    "    print(f\"  Filtered out {filtered_rows} rows older than August 12, 2024\")\n",
    "    \n",
    "    # Create time dimensions\n",
    "    print(\"- Creating time dimensions...\")\n",
    "    transformed_df['Year'] = transformed_df['InvoiceDate'].dt.year\n",
    "    transformed_df['Month'] = transformed_df['InvoiceDate'].dt.month\n",
    "    transformed_df['Quarter'] = transformed_df['InvoiceDate'].dt.quarter\n",
    "    transformed_df['Date'] = transformed_df['InvoiceDate'].dt.date\n",
    "    \n",
    "    print(f\"Final transformed rows: {len(transformed_df)}\")\n",
    "    logger.info(f\"Transformation complete. Rows: {initial_rows} -> {len(transformed_df)}\")\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "# Apply transformations\n",
    "transformed_data = transform_retail_data(retail_data)\n",
    "\n",
    "# Create customer dimension data\n",
    "def create_customer_dimension(df):\n",
    "    \"\"\"Create customer dimension table\"\"\"\n",
    "    print(\"- Creating Customer Dimension...\")\n",
    "    \n",
    "    customer_dim = df.groupby('customer_id').agg({\n",
    "        'Country': 'first',\n",
    "        'TotalSales': 'sum',\n",
    "        'InvoiceNo': 'nunique',\n",
    "        'Quantity': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_dim.columns = ['customer_id', 'Country', 'TotalPurchases', 'TotalInvoices', 'TotalQuantity']\n",
    "    customer_dim['CustomerSegment'] = pd.cut(customer_dim['TotalPurchases'], \n",
    "                                           bins=3, \n",
    "                                           labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    print(f\"  Created customer dimension with {len(customer_dim)} customers\")\n",
    "    return customer_dim\n",
    "\n",
    "# Create time dimension data\n",
    "def create_time_dimension(df):\n",
    "    \"\"\"Create time dimension table\"\"\"\n",
    "    print(\"- Creating Time Dimension...\")\n",
    "    \n",
    "    time_dim = df[['Date', 'Year', 'Month', 'Quarter']].drop_duplicates().reset_index(drop=True)\n",
    "    time_dim['TimeID'] = range(1, len(time_dim) + 1)\n",
    "    time_dim['MonthName'] = pd.to_datetime(time_dim['Date']).dt.month_name()\n",
    "    time_dim['Weekday'] = pd.to_datetime(time_dim['Date']).dt.day_name()\n",
    "    \n",
    "    print(f\"  Created time dimension with {len(time_dim)} time periods\")\n",
    "    return time_dim\n",
    "\n",
    "# Create product dimension data\n",
    "def create_product_dimension(df):\n",
    "    \"\"\"Create product dimension table\"\"\"\n",
    "    print(\"- Creating Product Dimension...\")\n",
    "    \n",
    "    product_dim = df[['StockCode', 'Description', 'Category', 'UnitPrice']].drop_duplicates().reset_index(drop=True)\n",
    "    product_dim['ProductID'] = range(1, len(product_dim) + 1)\n",
    "    product_dim['AvgPrice'] = product_dim.groupby('Category')['UnitPrice'].transform('mean')\n",
    "    \n",
    "    print(f\"  Created product dimension with {len(product_dim)} products\")\n",
    "    return product_dim\n",
    "\n",
    "# Create dimension tables\n",
    "customer_dim = create_customer_dimension(transformed_data)\n",
    "time_dim = create_time_dimension(transformed_data)\n",
    "product_dim = create_product_dimension(transformed_data)\n",
    "\n",
    "print(\"\\nTransformation Summary:\")\n",
    "print(f\"‚úì Fact table rows: {len(transformed_data)}\")\n",
    "print(f\"‚úì Customer dimension: {len(customer_dim)} records\")\n",
    "print(f\"‚úì Time dimension: {len(time_dim)} records\")\n",
    "print(f\"‚úì Product dimension: {len(product_dim)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e498459",
   "metadata": {},
   "source": [
    "## Step 3: Load Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e844af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 13:45:35,931 - INFO - Creating database schema\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Loading Data to Database (retail_dw.db)...\n",
      "‚úì Database schema created successfully\n",
      "- Loading dimension tables...\n",
      "  ‚úì Loaded 99 customers\n",
      "  ‚úì Loaded 261 time records\n",
      "  ‚úì Loaded 467 products\n",
      "- Creating sales fact table...\n",
      "  ‚úì Created sales fact with 467 records\n",
      "- Loading sales fact table...\n",
      "  ‚úì Loaded 467 sales records\n"
     ]
    }
   ],
   "source": [
    "def create_database_schema(db_path):\n",
    "    \"\"\"Create the data warehouse schema in SQLite\"\"\"\n",
    "    print(f\"\\nStep 3: Loading Data to Database ({db_path})...\")\n",
    "    logger.info(\"Creating database schema\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create Customer Dimension Table\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS CustomerDim (\n",
    "        customer_id INTEGER PRIMARY KEY,\n",
    "        Country TEXT NOT NULL,\n",
    "        TotalPurchases REAL,\n",
    "        TotalInvoices INTEGER,\n",
    "        TotalQuantity INTEGER,\n",
    "        CustomerSegment TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # Create Time Dimension Table\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS TimeDim (\n",
    "        TimeID INTEGER PRIMARY KEY,\n",
    "        Date DATE NOT NULL,\n",
    "        Year INTEGER,\n",
    "        Month INTEGER,\n",
    "        Quarter INTEGER,\n",
    "        MonthName TEXT,\n",
    "        Weekday TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # Create Product Dimension Table\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS ProductDim (\n",
    "        ProductID INTEGER PRIMARY KEY,\n",
    "        StockCode TEXT,\n",
    "        Description TEXT,\n",
    "        Category TEXT,\n",
    "        UnitPrice REAL,\n",
    "        AvgPrice REAL\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    # Create Sales Fact Table\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS SalesFact (\n",
    "        SalesID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        InvoiceNo TEXT,\n",
    "        customer_id INTEGER,\n",
    "        ProductID INTEGER,\n",
    "        TimeID INTEGER,\n",
    "        Quantity INTEGER,\n",
    "        UnitPrice REAL,\n",
    "        TotalSales REAL,\n",
    "        FOREIGN KEY (customer_id) REFERENCES CustomerDim(customer_id),\n",
    "        FOREIGN KEY (ProductID) REFERENCES ProductDim(ProductID),\n",
    "        FOREIGN KEY (TimeID) REFERENCES TimeDim(TimeID)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"‚úì Database schema created successfully\")\n",
    "    return conn\n",
    "\n",
    "def load_dimension_tables(conn, customer_dim, time_dim, product_dim):\n",
    "    \"\"\"Load data into dimension tables\"\"\"\n",
    "    print(\"- Loading dimension tables...\")\n",
    "    \n",
    "    # Load Customer Dimension\n",
    "    customer_dim.to_sql('CustomerDim', conn, if_exists='replace', index=False)\n",
    "    customer_count = conn.execute(\"SELECT COUNT(*) FROM CustomerDim\").fetchone()[0]\n",
    "    print(f\"  ‚úì Loaded {customer_count} customers\")\n",
    "    \n",
    "    # Load Time Dimension\n",
    "    time_dim.to_sql('TimeDim', conn, if_exists='replace', index=False)\n",
    "    time_count = conn.execute(\"SELECT COUNT(*) FROM TimeDim\").fetchone()[0]\n",
    "    print(f\"  ‚úì Loaded {time_count} time records\")\n",
    "    \n",
    "    # Load Product Dimension\n",
    "    product_dim.to_sql('ProductDim', conn, if_exists='replace', index=False)\n",
    "    product_count = conn.execute(\"SELECT COUNT(*) FROM ProductDim\").fetchone()[0]\n",
    "    print(f\"  ‚úì Loaded {product_count} products\")\n",
    "    \n",
    "    return customer_count, time_count, product_count\n",
    "\n",
    "def create_sales_fact(transformed_data, time_dim, product_dim):\n",
    "    \"\"\"Create sales fact table with proper foreign keys\"\"\"\n",
    "    print(\"- Creating sales fact table...\")\n",
    "    \n",
    "    # Merge with time dimension to get TimeID\n",
    "    fact_table = transformed_data.merge(\n",
    "        time_dim[['Date', 'TimeID']], \n",
    "        on='Date', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Merge with product dimension to get ProductID\n",
    "    fact_table = fact_table.merge(\n",
    "        product_dim[['StockCode', 'ProductID']], \n",
    "        on='StockCode', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Select only the columns needed for fact table\n",
    "    sales_fact = fact_table[[\n",
    "        'InvoiceNo', 'customer_id', 'ProductID', 'TimeID',\n",
    "        'Quantity', 'UnitPrice', 'TotalSales'\n",
    "    ]].copy()\n",
    "    \n",
    "    print(f\"  ‚úì Created sales fact with {len(sales_fact)} records\")\n",
    "    return sales_fact\n",
    "\n",
    "def load_fact_table(conn, sales_fact):\n",
    "    \"\"\"Load sales fact table\"\"\"\n",
    "    print(\"- Loading sales fact table...\")\n",
    "    \n",
    "    sales_fact.to_sql('SalesFact', conn, if_exists='replace', index=False)\n",
    "    fact_count = conn.execute(\"SELECT COUNT(*) FROM SalesFact\").fetchone()[0]\n",
    "    print(f\"  ‚úì Loaded {fact_count} sales records\")\n",
    "    \n",
    "    return fact_count\n",
    "\n",
    "# Execute the loading process\n",
    "db_path = 'retail_dw.db'\n",
    "conn = create_database_schema(db_path)\n",
    "\n",
    "# Load dimensions\n",
    "customer_count, time_count, product_count = load_dimension_tables(conn, customer_dim, time_dim, product_dim)\n",
    "\n",
    "# Create and load fact table\n",
    "sales_fact = create_sales_fact(transformed_data, time_dim, product_dim)\n",
    "fact_count = load_fact_table(conn, sales_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158712b3",
   "metadata": {},
   "source": [
    "## Step 4: ETL Function and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6c7ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 13:46:22,783 - INFO - Starting full ETL process\n",
      "2025-08-14 13:46:22,787 - INFO - Extract phase started\n",
      "2025-08-14 13:46:22,856 - INFO - Extract completed: 1000 rows\n",
      "2025-08-14 13:46:22,856 - INFO - Transform phase started\n",
      "2025-08-14 13:46:22,856 - INFO - Starting data transformation\n",
      "2025-08-14 13:46:22,887 - INFO - Transformation complete. Rows: 1000 -> 447\n",
      "2025-08-14 13:46:22,915 - INFO - Transform completed: 447 fact rows\n",
      "2025-08-14 13:46:22,919 - INFO - Load phase started\n",
      "2025-08-14 13:46:22,922 - INFO - Creating database schema\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPLETE ETL PROCESS EXECUTION\n",
      "============================================================\n",
      "\n",
      "üîÑ EXTRACT PHASE\n",
      "‚úÖ Extracted 1000 rows\n",
      "\n",
      "üîÑ TRANSFORM PHASE\n",
      "\n",
      "Step 2: Transforming Data...\n",
      "Initial rows: 1000\n",
      "- Handling missing values...\n",
      "  Missing values before: 50, after: 0\n",
      "- Converting data types...\n",
      "- Removing outliers...\n",
      "  Removed 45 rows with outliers\n",
      "- Calculating TotalSales...\n",
      "- Filtering for last year's data...\n",
      "  Filtered out 458 rows older than August 12, 2024\n",
      "- Creating time dimensions...\n",
      "Final transformed rows: 447\n",
      "- Creating Customer Dimension...\n",
      "  Created customer dimension with 98 customers\n",
      "- Creating Time Dimension...\n",
      "  Created time dimension with 259 time periods\n",
      "- Creating Product Dimension...\n",
      "  Created product dimension with 447 products\n",
      "‚úÖ Transformed to 447 fact rows\n",
      "\n",
      "üîÑ LOAD PHASE\n",
      "\n",
      "Step 3: Loading Data to Database (retail_dw.db)...\n",
      "‚úì Database schema created successfully\n",
      "- Loading dimension tables...\n",
      "  ‚úì Loaded 98 customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 13:46:23,056 - INFO - Load completed: 447 sales records\n",
      "2025-08-14 13:46:23,058 - INFO - ETL process completed successfully in 0.28 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Loaded 259 time records\n",
      "  ‚úì Loaded 447 products\n",
      "- Creating sales fact table...\n",
      "  ‚úì Created sales fact with 447 records\n",
      "- Loading sales fact table...\n",
      "  ‚úì Loaded 447 sales records\n",
      "‚úÖ Loaded 447 sales records\n",
      "\n",
      "============================================================\n",
      "ETL PROCESS SUMMARY\n",
      "============================================================\n",
      "‚è±Ô∏è  Duration: 0.28 seconds\n",
      "üì• Extracted: 1,000 rows\n",
      "üîß Transformed: 447 rows\n",
      "üì§ Loaded:\n",
      "   - Sales Facts: 447\n",
      "   - Customers: 98\n",
      "   - Time Periods: 259\n",
      "   - Products: 447\n",
      "üíæ Database: retail_dw.db\n",
      "‚úÖ ETL Process Completed Successfully!\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "üìä Table Record Counts:\n",
      "   CustomerDim: 98 records\n",
      "   TimeDim: 259 records\n",
      "   ProductDim: 447 records\n",
      "   SalesFact: 447 records\n",
      "\n",
      "üîó Referential Integrity Checks:\n",
      "   Orphaned Customer Records: 0\n",
      "   Orphaned Product Records: 0\n",
      "   Orphaned Time Records: 0\n",
      "\n",
      "üìà Data Quality Metrics:\n",
      "   Sales Range: $16.00 - $4951.50 (Avg: $1244.93)\n",
      "   Date Range: 2024-08-12 to 2025-08-11\n",
      "\n",
      "üåç Top 5 Countries by Sales:\n",
      "   UK: $106,403.90 (81 transactions)\n",
      "   France: $92,112.19 (83 transactions)\n",
      "   Italy: $71,126.60 (54 transactions)\n",
      "   USA: $56,530.92 (49 transactions)\n",
      "   Germany: $54,768.17 (43 transactions)\n",
      "\n",
      "‚úÖ Data Quality Verification Complete!\n",
      "\n",
      "============================================================\n",
      "TASK 2 COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "üìÅ Files Created:\n",
      "   - synthetic_retail_data.csv (source data)\n",
      "   - retail_dw.db (data warehouse)\n",
      "   - Complete ETL process implemented\n",
      "   - Data quality verified\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def full_etl_process(num_rows=1000, db_path='retail_dw.db'):\n",
    "    \"\"\"\n",
    "    Complete ETL process function with comprehensive logging\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPLETE ETL PROCESS EXECUTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"Starting full ETL process\")\n",
    "    \n",
    "    try:\n",
    "        # EXTRACT\n",
    "        print(\"\\nüîÑ EXTRACT PHASE\")\n",
    "        logger.info(\"Extract phase started\")\n",
    "        raw_data = generate_synthetic_retail_data(num_rows)\n",
    "        extract_rows = len(raw_data)\n",
    "        print(f\"‚úÖ Extracted {extract_rows} rows\")\n",
    "        logger.info(f\"Extract completed: {extract_rows} rows\")\n",
    "        \n",
    "        # TRANSFORM\n",
    "        print(\"\\nüîÑ TRANSFORM PHASE\")\n",
    "        logger.info(\"Transform phase started\")\n",
    "        transformed_data = transform_retail_data(raw_data)\n",
    "        transform_rows = len(transformed_data)\n",
    "        \n",
    "        # Create dimensions\n",
    "        customer_dim = create_customer_dimension(transformed_data)\n",
    "        time_dim = create_time_dimension(transformed_data)\n",
    "        product_dim = create_product_dimension(transformed_data)\n",
    "        \n",
    "        print(f\"‚úÖ Transformed to {transform_rows} fact rows\")\n",
    "        logger.info(f\"Transform completed: {transform_rows} fact rows\")\n",
    "        \n",
    "        # LOAD\n",
    "        print(\"\\nüîÑ LOAD PHASE\")\n",
    "        logger.info(\"Load phase started\")\n",
    "        conn = create_database_schema(db_path)\n",
    "        \n",
    "        # Load dimensions\n",
    "        customer_count, time_count, product_count = load_dimension_tables(conn, customer_dim, time_dim, product_dim)\n",
    "        \n",
    "        # Load fact\n",
    "        sales_fact = create_sales_fact(transformed_data, time_dim, product_dim)\n",
    "        fact_count = load_fact_table(conn, sales_fact)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {fact_count} sales records\")\n",
    "        logger.info(f\"Load completed: {fact_count} sales records\")\n",
    "        \n",
    "        # Summary\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ETL PROCESS SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {duration:.2f} seconds\")\n",
    "        print(f\"üì• Extracted: {extract_rows:,} rows\")\n",
    "        print(f\"üîß Transformed: {transform_rows:,} rows\")\n",
    "        print(f\"üì§ Loaded:\")\n",
    "        print(f\"   - Sales Facts: {fact_count:,}\")\n",
    "        print(f\"   - Customers: {customer_count:,}\")\n",
    "        print(f\"   - Time Periods: {time_count:,}\")\n",
    "        print(f\"   - Products: {product_count:,}\")\n",
    "        print(f\"üíæ Database: {db_path}\")\n",
    "        print(f\"‚úÖ ETL Process Completed Successfully!\")\n",
    "        \n",
    "        logger.info(f\"ETL process completed successfully in {duration:.2f} seconds\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'duration': duration,\n",
    "            'extracted_rows': extract_rows,\n",
    "            'transformed_rows': transform_rows,\n",
    "            'loaded_facts': fact_count,\n",
    "            'loaded_customers': customer_count,\n",
    "            'loaded_time_periods': time_count,\n",
    "            'loaded_products': product_count\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ETL process failed: {str(e)}\")\n",
    "        print(f\"‚ùå ETL Process Failed: {str(e)}\")\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Execute the complete ETL process\n",
    "etl_result = full_etl_process(1000, 'retail_dw.db')\n",
    "\n",
    "# ## Data Quality Verification\n",
    "\n",
    "def verify_data_quality(db_path='retail_dw.db'):\n",
    "    \"\"\"Verify the quality of loaded data\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATA QUALITY VERIFICATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Check table counts\n",
    "    print(\"\\nüìä Table Record Counts:\")\n",
    "    tables = ['CustomerDim', 'TimeDim', 'ProductDim', 'SalesFact']\n",
    "    for table in tables:\n",
    "        count = conn.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "        print(f\"   {table}: {count:,} records\")\n",
    "    \n",
    "    # Check for referential integrity\n",
    "    print(\"\\nüîó Referential Integrity Checks:\")\n",
    "    \n",
    "    # Check for orphaned records in fact table\n",
    "    orphaned_customers = conn.execute('''\n",
    "        SELECT COUNT(*) FROM SalesFact s \n",
    "        LEFT JOIN CustomerDim c ON s.customer_id = c.customer_id \n",
    "        WHERE c.customer_id IS NULL\n",
    "    ''').fetchone()[0]\n",
    "    \n",
    "    orphaned_products = conn.execute('''\n",
    "        SELECT COUNT(*) FROM SalesFact s \n",
    "        LEFT JOIN ProductDim p ON s.ProductID = p.ProductID \n",
    "        WHERE p.ProductID IS NULL\n",
    "    ''').fetchone()[0]\n",
    "    \n",
    "    orphaned_time = conn.execute('''\n",
    "        SELECT COUNT(*) FROM SalesFact s \n",
    "        LEFT JOIN TimeDim t ON s.TimeID = t.TimeID \n",
    "        WHERE t.TimeID IS NULL\n",
    "    ''').fetchone()[0]\n",
    "    \n",
    "    print(f\"   Orphaned Customer Records: {orphaned_customers}\")\n",
    "    print(f\"   Orphaned Product Records: {orphaned_products}\")\n",
    "    print(f\"   Orphaned Time Records: {orphaned_time}\")\n",
    "    \n",
    "    # Check data ranges\n",
    "    print(\"\\nüìà Data Quality Metrics:\")\n",
    "    \n",
    "    # Sales amount range\n",
    "    sales_stats = conn.execute('''\n",
    "        SELECT MIN(TotalSales), MAX(TotalSales), AVG(TotalSales) FROM SalesFact\n",
    "    ''').fetchone()\n",
    "    print(f\"   Sales Range: ${sales_stats[0]:.2f} - ${sales_stats[1]:.2f} (Avg: ${sales_stats[2]:.2f})\")\n",
    "    \n",
    "    # Date range\n",
    "    date_range = conn.execute('''\n",
    "        SELECT MIN(Date), MAX(Date) FROM TimeDim\n",
    "    ''').fetchone()\n",
    "    print(f\"   Date Range: {date_range[0]} to {date_range[1]}\")\n",
    "    \n",
    "    # Country distribution\n",
    "    print(\"\\nüåç Top 5 Countries by Sales:\")\n",
    "    top_countries = conn.execute('''\n",
    "        SELECT c.Country, SUM(s.TotalSales) as TotalSales, COUNT(*) as TransactionCount\n",
    "        FROM SalesFact s\n",
    "        JOIN CustomerDim c ON s.customer_id = c.customer_id\n",
    "        GROUP BY c.Country\n",
    "        ORDER BY TotalSales DESC\n",
    "        LIMIT 5\n",
    "    ''').fetchall()\n",
    "    \n",
    "    for country, sales, count in top_countries:\n",
    "        print(f\"   {country}: ${sales:,.2f} ({count:,} transactions)\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data Quality Verification Complete!\")\n",
    "\n",
    "# Run data quality verification\n",
    "verify_data_quality('retail_dw.db')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TASK 2 COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"üìÅ Files Created:\")\n",
    "print(\"   - synthetic_retail_data.csv (source data)\")\n",
    "print(\"   - retail_dw.db (data warehouse)\")\n",
    "print(\"   - Complete ETL process implemented\")\n",
    "print(\"   - Data quality verified\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
